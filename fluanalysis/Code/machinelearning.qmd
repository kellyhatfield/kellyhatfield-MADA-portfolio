---
title: "Machine Learning"
author: "Kelly Hatfield"
format: html
editor: visual
---

## Step 1: Opening Data and Loading Packages

```{r}
library(here)
library (tidyverse)
library (tidymodels)
library(rpart)
library("rpart.plot")
library("glmnet")
library("ranger")
library(vip)


FinalDataML<- readRDS(here("fluanalysis","Data","FinalDataML.Rds"))

```

## Step 2: Machine Learning

### Data Setup and Null Model Performance

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(123)


# Put 70/30 of the data into the training set 
data_split <- initial_split(FinalDataML, prop = 7/10)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)


# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)


#5-fold cross-validation, 5 times repeated
fold_ds<- vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)

#Recipe for the data and fitting 
data_recipe <- recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) 

null_recipe <- recipe(BodyTemp ~ 1, data = train_data) %>%
  step_dummy(all_nominal(), -all_outcomes()) 

#linear model
ln_model <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")

#Workflow
null_flow <- workflow() %>% add_model(ln_model) %>% add_recipe(null_recipe)

#look at model
null_fit <- null_flow %>% fit(data=train_data) %>% fit_resamples(resamples=fold_ds)
null_metrics<- collect_metrics(null_fit)
null_metrics

#RMSE: 1.21

```

### Model tuning and fitting

#### Tree

```{r}

#Model
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")


#Grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
#create workflow
tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(BodyTemp ~ .)


#Tuning grid cross validation
tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = fold_ds,
    grid = tree_grid
    )
tree_res %>% 
  collect_metrics()


#Look at the best model 
tree_res %>%
  show_best()

#rmse = 1.199

#Select best tree
best_tree <- tree_res %>%
  select_best(n=1)

#Final model from best tree
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)


#Fit 
final_fit <- 
  final_wf %>%
  fit(train_data) 


final_fit

#Plot final fit
rpart.plot(extract_fit_parsnip(final_fit)$fit)

```

#### Lasso

```{r}

#The steps (block of code) you should have here are 1) model specification, 2) workflow definition, 3) tuning grid specification and 4) tuning using cross-validation and the tune_grid() function.



#Build  model
lasso_mod <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

#Create workflow using data recipe from above
lasso_workflow <- 
  workflow() %>% 
  add_model(lasso_mod) %>% 
  add_recipe(data_recipe)

# tuning grid
lasso_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
#Bottom 5 penalty values
lasso_grid %>% top_n(-5)
#Top 5 penalty values
lasso_grid %>% top_n(5)


#Using tuning grids
lr_res <- 
  lasso_workflow %>% 
  tune_grid(resamples = fold_ds,
            grid = lasso_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = NULL)

lr_res %>% collect_metrics()
lr_res %>% show_best()

#Selects best performing model
best_lasso <- lr_res %>% select_best()

#rmse = 1.182 -- 1.1815

#Final Model
lasso_final_wf <- 
  lasso_workflow %>% finalize_workflow(best_lasso)
lasso_final_fit <- lasso_final_wf %>% fit(train_data) 

#plot
x <- extract_fit_engine(lasso_final_fit)
plot(x, "lambda")
```

#### Random Forest

```{r}
#The steps (block of code) you should have here are 1) model specification, 2) workflow definition, 3) tuning grid specification and 4) tuning using cross-validation and the tune_grid() function.


#Build 
cores <- parallel::detectCores()
cores
randomforest_mod <-  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", importance = "impurity", num.threads = cores) %>% set_mode("regression")

#Workflow
randomforest_wf <-  workflow() %>% add_model(randomforest_mod) %>% add_recipe(data_recipe)

#Tune
extract_parameter_set_dials(randomforest_mod)
#Tune grid
randomforest_res <- randomforest_wf %>%  tune_grid(fold_ds, grid = 25, control = control_grid(save_pred = TRUE), metrics = NULL)

#Best forest
randomforest_res %>% show_best()
randomforest_best <- randomforest_res %>% select_best()

# workflow
randomforest_fwf<- randomforest_wf %>% finalize_workflow(randomforest_best)

#Final fit
ranforest_fin <- randomforest_fwf %>% fit(train_data)
ranforest_fin %>% extract_fit_parsnip() %>% vip(num_features = 28)

fun <- extract_fit_engine(ranforest_fin)
vip(fun)

#rmse 1.19
```

### Final Model

We select LASSO model because it had the highest RMSE.

We will run the LASSO model on the split data

```{r}
lasso_test_data <- 
  lasso_final_wf %>%
  last_fit(data_split) 
lasso_test_data %>%
   collect_metrics()

#RMSE 1.100

```
